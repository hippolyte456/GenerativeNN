{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook a pour but d'explorer différentes méthodes pour utiliser et fine-tuner des réseaux préentrainés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453a6fd0cae548608a36902dfca07764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data\n"
     ]
    }
   ],
   "source": [
    "#on load un dataset d'image depuis pytorch\n",
    "dataset = dset.CIFAR10(root='data',\n",
    "                          download=True,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.Resize(64),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on affiche une image du dataset avec matplotlib\n",
    "lenD = len(dataset)\n",
    "print(lenD)\n",
    "i = random.randint(0, lenD)\n",
    "img = dataset[i][0]\n",
    "Vimg = img.numpy().transpose(1,2,0)\n",
    "plt.imshow(Vimg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/ Utiliser des réseaux fournis avec Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.models.resnet.ResNet'>\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# Best available weights (currently alias for IMAGENET1K_V2)\n",
    "# Note that these weights may change across versions\n",
    "my_model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "print(type(my_model))\n",
    "\n",
    "# # Old weights with accuracy 76.130%\n",
    "# resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "# # New weights with accuracy 80.858%\n",
    "# resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "# # Strings are also supported\n",
    "# resnet50(weights=\"IMAGENET1K_V2\")\n",
    "# # No weights - random initialization\n",
    "# resnet50(weights=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... il est important de preprocesser de la bonne manière en focntion du modèle utiliser : en pytorch c'est simple il suffit de récupérer la fonction transforms() qui doit être présent comme attribut du \"weight\" choisi pour le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Weight Transforms\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "\n",
    "# Apply it to the input image\n",
    "img_transformed = preprocess(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affichage de l'image transformée et de l'image dans le meme cadre \n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].set_title('Original')\n",
    "ax[0].imshow(Vimg)\n",
    "ax[1].set_title('Transformed')\n",
    "ax[1].imshow(img_transformed.numpy().transpose(1,2,0))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...ne pas oublier que le modele charger peut avoir un comportement différent en train et eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to eval mode\n",
    "my_model.eval()\n",
    "#my_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List available models\n",
    "# all_models = list_models()\n",
    "# classification_models = list_models(module=torchvision.models)\n",
    "\n",
    "# # Initialize models\n",
    "# m1 = get_model(\"mobilenet_v3_large\", weights=None)\n",
    "# m2 = get_model(\"quantized_mobilenet_v3_large\", weights=\"DEFAULT\")\n",
    "\n",
    "# # Fetch weights\n",
    "# weights = get_weight(\"MobileNet_V3_Large_QuantizedWeights.DEFAULT\")\n",
    "# assert weights == MobileNet_V3_Large_QuantizedWeights.DEFAULT\n",
    "\n",
    "# weights_enum = get_model_weights(\"quantized_mobilenet_v3_large\")\n",
    "# assert weights_enum == MobileNet_V3_Large_QuantizedWeights\n",
    "\n",
    "# weights_enum2 = get_model_weights(torchvision.models.quantization.mobilenet_v3_large)\n",
    "# assert weights_enum == weights_enum2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchviz\n",
    "#on visualise le modèle avec torchviz\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "y = my_model(x)\n",
    "y.detach_()\n",
    "#on trnasforme en numpy\n",
    "y = y.numpy()\n",
    "y.reshape((20,50))\n",
    "\n",
    "#on affiche l'image avec matplotlib\n",
    "#plt.imshow(y)\n",
    "# g = torchviz.make_dot(y.mean(), params=dict(my_model.named_parameters()))\n",
    "# g.view()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II/ Utiliser du code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "baacd7ded0742aa8408bda3ed6ced71320ba4869ece4e05e453d0cf31ed1376f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

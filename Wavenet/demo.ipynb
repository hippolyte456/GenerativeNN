{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch \n",
    "\n",
    "df=pd.read_csv(\"../../Data/data_wavenet/train_clean.csv\")\n",
    "df['signal']=df['signal']/df['signal'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['open_channels'].max()\n",
    "df['open_channels'].min()\n",
    "#nombre de samples\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create time series dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_184201/3899109925.py:2: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  def __init__(self, df:pd.Series(), seqLen=10):\n"
     ]
    }
   ],
   "source": [
    "class TimesereisDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df:pd.Series(), seqLen=10):\n",
    "        super().__init__()\n",
    "        self.df=df\n",
    "        self.seqLen=seqLen\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]-self.seqLen-1\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        x=self.df.iloc[index:index+self.seqLen,1].values\n",
    "        y=self.df.iloc[index+self.seqLen-1,2] \n",
    "        return x,y   \n",
    "seqLen=50\n",
    "timeSeriesDataset = TimesereisDataset(df,seqLen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batchsize, batchnorm ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNumbers=int(len(timeSeriesDataset)*0.9)\n",
    "trainDataset,testDataset=torch.utils.data.random_split(timeSeriesDataset,[trainNumbers,len(timeSeriesDataset)-trainNumbers])\n",
    "trainDataLoader=torch.utils.data.DataLoader(trainDataset,batch_size=8,shuffle=True)\n",
    "testDataLoader=torch.utils.data.DataLoader(testDataset,batch_size=8,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4499954"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainNumbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 0it [00:00, ?it/s]/home/hippo/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n",
      "Validation: 201it [00:05, 38.21it/s]\n",
      "Training: 3it [00:06,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for step 0 : 2.418416332842699  :  Accuracy: 2.9228855721393034 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 201it [00:04, 45.41it/s]\n",
      "Training: 503it [00:44,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for step 500 : 2.289049127208653  :  Accuracy: 25.99502487562189 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 201it [00:04, 46.53it/s]\n",
      "Training: 1002it [01:21,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for step 1000 : 2.2896262294617458  :  Accuracy: 25.808457711442784 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 201it [00:05, 38.46it/s]\n",
      "Training: 1502it [02:02,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for step 1500 : 2.305651125030138  :  Accuracy: 23.818407960199004 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 201it [00:05, 36.04it/s]\n",
      "Training: 2004it [02:44,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for step 2000 : 2.2882527225646214  :  Accuracy: 25.559701492537314 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 201it [00:04, 41.04it/s]\n",
      "Training: 2502it [03:24,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for step 2500 : 2.277089779649801  :  Accuracy: 27.05223880597015 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 201it [00:04, 47.00it/s]\n",
      "Training: 3004it [04:02,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for step 3000 : 2.2970915445640907  :  Accuracy: 24.129353233830845 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 201it [00:05, 36.27it/s]Exception ignored in: <generator object tqdm.__iter__ at 0x7f70c75aaa50>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hippo/anaconda3/lib/python3.9/site-packages/tqdm/std.py\", line 1193, in __iter__\n",
      "    self.close()\n",
      "  File \"/home/hippo/anaconda3/lib/python3.9/site-packages/tqdm/std.py\", line 1299, in close\n",
      "    self.display(pos=0)\n",
      "  File \"/home/hippo/anaconda3/lib/python3.9/site-packages/tqdm/std.py\", line 1492, in display\n",
      "    self.sp(self.__str__() if msg is None else msg)\n",
      "  File \"/home/hippo/anaconda3/lib/python3.9/site-packages/tqdm/std.py\", line 348, in print_status\n",
      "    fp_write('\\r' + s + (' ' * max(last_len[0] - len_s, 0)))\n",
      "  File \"/home/hippo/anaconda3/lib/python3.9/site-packages/tqdm/std.py\", line 342, in fp_write\n",
      "    fp_flush()\n",
      "  File \"/home/hippo/anaconda3/lib/python3.9/site-packages/tqdm/utils.py\", line 142, in inner\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/hippo/anaconda3/lib/python3.9/site-packages/ipykernel/iostream.py\", line 468, in flush\n",
      "    if not evt.wait(self.flush_timeout):\n",
      "  File \"/home/hippo/anaconda3/lib/python3.9/threading.py\", line 574, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"/home/hippo/anaconda3/lib/python3.9/threading.py\", line 316, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "KeyboardInterrupt: \n",
      "Training: 3503it [04:43,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for step 3500 : 2.284733992310899  :  Accuracy: 26.99004975124378 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 3890it [05:11, 12.50it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hippo/Documents/GenerativeNN/Wavenet/demo.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hippo/Documents/GenerativeNN/Wavenet/demo.ipynb#W5sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hippo/Documents/GenerativeNN/Wavenet/demo.ipynb#W5sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hippo/Documents/GenerativeNN/Wavenet/demo.ipynb#W5sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hippo/Documents/GenerativeNN/Wavenet/demo.ipynb#W5sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mif\u001b[39;00m step\u001b[39m%\u001b[39mglobalStep\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hippo/Documents/GenerativeNN/Wavenet/demo.ipynb#W5sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m    \u001b[39m# scheduler.step()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hippo/Documents/GenerativeNN/Wavenet/demo.ipynb#W5sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m    \u001b[39m# print(output.detach().numpy())\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hippo/Documents/GenerativeNN/Wavenet/demo.ipynb#W5sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m    \u001b[39m# print(y_train.numpy())\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hippo/Documents/GenerativeNN/Wavenet/demo.ipynb#W5sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m    \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:67\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     66\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     25\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> 26\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/adamw.py:73\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39m# Perform stepweight decay\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m p\u001b[39m.\u001b[39;49mmul_(\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m group[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m*\u001b[39;49m group[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     75\u001b[0m \u001b[39m# Perform optimization step\u001b[39;00m\n\u001b[1;32m     76\u001b[0m grad \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mgrad\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from Wavenet import WaveNet,WaveNetClassifier\n",
    "from tqdm import tqdm \n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=\"cpu\"\n",
    "wavenetClassifierModel=WaveNetClassifier(seqLen,df['open_channels'].max()+1)\n",
    "wavenetClassifierModel.to(device)\n",
    "\n",
    "wavenetClassifierModel.train()\n",
    "\n",
    "optimizer=torch.optim.AdamW(wavenetClassifierModel.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "lossFunction = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def calc_accuracy(Out,Y):\n",
    "    max_vals, max_indices = torch.max(Out,1)\n",
    "    train_acc = (max_indices == Y).sum().item()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "  \n",
    "epochs=1\n",
    "globalStep=500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, (x_train,y_train) in tqdm(enumerate(trainDataLoader),desc=\"Training\"):\n",
    "         x_train = torch.unsqueeze(x_train,dim=1).float()\n",
    "         x_train.to(device)\n",
    "         y_train.to(device)\n",
    "         output=wavenetClassifierModel(x_train)\n",
    "         output = torch.squeeze(output,dim=1)\n",
    "\n",
    "         loss= lossFunction(output,y_train)\n",
    "         optimizer.zero_grad()\n",
    "         loss.backward()\n",
    "         optimizer.step()\n",
    "         if step%globalStep==0:\n",
    "            # scheduler.step()\n",
    "            # print(output.detach().numpy())\n",
    "            # print(y_train.numpy())\n",
    "            with torch.no_grad():\n",
    "                accuracy=0\n",
    "                loss=0\n",
    "                for stepTest, (x_test,y_test) in tqdm(enumerate(testDataLoader),desc=\"Validation\"):\n",
    "                    x_test.to(device)\n",
    "                    y_test.to(device)\n",
    "                    x_test = torch.unsqueeze(x_test,dim=1).float()\n",
    "                    output=wavenetClassifierModel(x_test)\n",
    "                    output = torch.squeeze(output,dim=1)\n",
    "                    accuracy+=calc_accuracy(output,y_test)*100\n",
    "                    loss+= lossFunction(output,y_test).item()\n",
    "                    if stepTest>200:\n",
    "                        break\n",
    "            print(f\"loss for step {step} : {loss/stepTest}  :  Accuracy: {accuracy/stepTest} %\")\n",
    "\n",
    "         \n",
    "    print(f\"epch {epoch}\")\n",
    "         "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "366252978e52bb2df929d3934aeb3ff29dfa67e45e575a59a0b0194f7beef5a9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
